{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load English language\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecff4d5",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/cnn-sentiment-analysis-9b1771e7cdd6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca7fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"kaggle_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.dropna(inplace=True, subset=['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['word_counts'] = tweets.text_clean.str.split().map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce809f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df , test_df  = train_test_split(tweets, test_size = 0.2, random_state = SEED, shuffle = True)\n",
    "val_df , test_df  = train_test_split(test_df, test_size = 0.5, random_state = SEED, shuffle = True)\n",
    "\n",
    "print(train_df.shape , test_df.shape , val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59515696",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', batch_first = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c157568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 # Load pretrained embeddings\n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1afacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    ''' Define network architecture and forward path. '''\n",
    "    def __init__(self, vocab_size, \n",
    "                 vector_size, n_filters, \n",
    "                 filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Create word embeddings from the input words     \n",
    "        self.embedding = nn.Embedding(vocab_size, vector_size, \n",
    "                                      padding_idx = pad_idx)\n",
    "        \n",
    "        # Specify convolutions with filters of different sizes (fs)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1, \n",
    "                                  out_channels = n_filters, \n",
    "                                  kernel_size = (fs, vector_size)) \n",
    "                                    for fs in filter_sizes])\n",
    "        \n",
    "        # Add a fully connected layer for final predicitons\n",
    "        self.linear = nn.Linear(len(filter_sizes) \\\n",
    "                      * n_filters, output_dim)\n",
    "        \n",
    "        # Drop some of the nodes to increase robustness in training\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "         '''Forward path of the network.'''       \n",
    "        # Get word embeddings and formt them for convolutions\n",
    "        embedded = self.embedding(text).unsqueeze(1)\n",
    "        \n",
    "        # Perform convolutions and apply activation functions\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) \n",
    "                  for conv in self.convs]\n",
    "            \n",
    "        # Pooling layer to reduce dimensionality    \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) \n",
    "                  for conv in conved]\n",
    "        \n",
    "        # Dropout layer\n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        return self.linear(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ffc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights with pre-trained embeddings\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "# Zero the initial weights of the UNKnown and padding tokens.\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "# The string token used as padding. Default: “<pad>”.\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ec8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def accuracy(preds, y):\n",
    "    \"\"\" Return accuracy per batch. \"\"\"\n",
    "    correct = (torch.round(torch.sigmoid(preds)) == y).float() \n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    '''Track training time. '''\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ebd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    '''Train the model with specified data, optimizer, and loss function. '''\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # Reset the gradient to not use them in multiple passes \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = accuracy(predictions, batch.label)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record accuracy and loss\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    '''Evaluate model performance. '''\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # Turm off dropout while evaluating\n",
    "    model.eval()\n",
    "    \n",
    "    # No need to backprop in eval\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c2c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ee5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "tr_loss = []\n",
    "tr_acc = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    # Calculate training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get epoch losses and accuracies \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Save training metrics\n",
    "    val_loss.append(valid_loss)\n",
    "    val_acc.append(valid_acc)\n",
    "    tr_loss.append(train_loss)\n",
    "    tr_acc.append(train_acc)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'CNN-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:2} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215cf019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].plot(val_loss, label='Validation loss')\n",
    "ax[0].plot(tr_loss, label='Training loss')\n",
    "ax[0].set_title('Losses')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(val_acc, label='Validation accuracy')\n",
    "ax[1].plot(tr_acc, label='Training accuracy')\n",
    "ax[1].set_title('Accuracies')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739242cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('CNN-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d503abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(model, sentence, min_len = 5):\n",
    "    '''Predict user-defined review sentiment.'''\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    # Map words to word embeddings\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    # Get predicitons\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
